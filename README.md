
# üöÄ NeuroPilot: Advanced Model Finetuning Toolkit

NeuroPilot is a powerful framework designed to accelerate and simplify the process of finetuning large language models. With support for distributed training, mixed precision, hyperparameter optimization, and advanced quantization techniques, NeuroPilot empowers machine learning engineers to train and optimize models with ease and efficiency.

Whether you're working on training a complex model or optimizing a pre-trained model like Phi-2, NeuroPilot offers the flexibility and power you need to achieve superior results.

![NeuroPilot](./assets/neuropilot-banner.png)

---

## üìã Project Overview

NeuroPilot is an intuitive and high-performance toolkit aimed at enhancing the finetuning process for language models. By incorporating powerful features such as Optuna-based hyperparameter tuning, mixed precision training, and flexible quantization options (4-bit, 8-bit), NeuroPilot ensures an efficient, scalable, and optimal model training workflow. The toolkit is designed to handle the computational demands of large-scale machine learning projects and is flexible enough for various model architectures and configurations.

---

## ‚ú® Key Features

- **Distributed Training**: Easily scale training across multiple GPUs or machines.
- **Mixed Precision Training**: Supports FP16 and BF16 for faster, memory-efficient training.
- **Quantization**: Save memory and increase efficiency with 4-bit and 8-bit quantization.
- **Hyperparameter Optimization**: Automatically optimize hyperparameters using Optuna.
- **Data Augmentation**: Built-in support for augmenting training data to enhance model robustness.
- **Advanced Text Generation**: Use various decoding strategies for high-quality text generation.
- **Streaming Generation**: Real-time token-by-token generation during inference.
- **Comprehensive Logging**: Full support for logging training progress, errors, and performance metrics.

---

## üõ† Installation

To install NeuroPilot and start using it, follow these steps:

```bash
git clone https://github.com/CEORocks/NeuroPilot.git
cd NeuroPilot
pip install -r requirements.txt


Ensure that you have CUDA installed for GPU acceleration.

---

## üöÄ Usage

### Training Your Model

Run the following command to start training your model:

```bash
python train.py \
  --model_path microsoft/phi-2 \
  --dataset_path your_dataset.json \
  --output_dir ./results \
  --batch_size 1 \
  --grad_accum_steps 4
```

For more advanced training options, such as mixed precision and distributed training, use the following:

```bash
python train.py \
  --model_path microsoft/phi-2 \
  --dataset_path your_dataset.json \
  --output_dir ./results \
  --mixed_precision bf16 \
  --load_in_4bit \
  --distributed \
  --tune_hyperparams \
  --n_trials 10 \
  --augment_data
```

### Text Generation

Generate text using the finetuned model:

```bash
python generate.py \
  --model_path ./results/final_model \
  --input_text "Your prompt here" \
  --max_length 200
```

For more advanced text generation with control over randomness:

```bash
python generate.py \
  --model_path ./results/final_model \
  --input_file inputs.json \
  --output_file outputs.json \
  --temperature 0.7 \
  --top_p 0.95 \
  --top_k 50 \
  --do_sample \
  --streaming \
  --mixed_precision
```

---

## ‚öôÔ∏è Advanced Configuration

### Training Arguments

- `--mixed_precision`: Choose between 'no', 'fp16', or 'bf16'.
- `--load_in_4bit`: Enable 4-bit quantization for efficient training.
- `--load_in_8bit`: Enable 8-bit quantization.
- `--distributed`: Enable distributed training across multiple GPUs or nodes.
- `--tune_hyperparams`: Enable automatic hyperparameter tuning using Optuna.
- `--augment_data`: Enable data augmentation during training.
- `--max_seq_length`: Set the maximum sequence length (default: 2048).
- `--learning_rate`: Set the learning rate (default: 2e-4).
- `--batch_size`: Set the batch size for each device.
- `--grad_accum_steps`: Set the number of gradient accumulation steps.

### Generation Arguments

- `--temperature`: Controls the randomness of the text generation (0.0-1.0).
- `--top_k`: Top-k sampling parameter.
- `--top_p`: Nucleus sampling parameter.
- `--num_beams`: Number of beams for beam search.
- `--repetition_penalty`: Penalizes repeated tokens during generation.
- `--streaming`: Enables token-by-token generation.
- `--mixed_precision`: Use mixed precision for inference.
- `--load_in_4bit`: Enable 4-bit quantization for memory-efficient inference.

---

## üéØ Example Use Cases

### 1. **Distributed Training with Mixed Precision**

```bash
python train.py \
  --model_path microsoft/phi-2 \
  --dataset_path your_dataset.json \
  --distributed \
  --mixed_precision bf16 \
  --batch_size 2 \
  --grad_accum_steps 4 \
  --learning_rate 2e-4 \
  --max_steps 10000
```

### 2. **Hyperparameter Optimization with Optuna**

```bash
python train.py \
  --model_path microsoft/phi-2 \
  --dataset_path your_dataset.json \
  --tune_hyperparams \
  --n_trials 20 \
  --load_in_4bit
```

### 3. **Batch Generation with Advanced Sampling**

```bash
python generate.py \
  --model_path ./results/final_model \
  --input_file inputs.json \
  --output_file outputs.json \
  --batch_size 4 \
  --temperature 0.7 \
  --top_p 0.95 \
  --do_sample \
  --mixed_precision
```

---

## ü§ù Contributing

We welcome contributions to NeuroPilot! Please follow these steps to contribute:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature-name`)
3. Commit your changes (`git commit -am 'Add new feature'`)
4. Push to the branch (`git push origin feature-name`)
5. Open a pull request

---

## üìÑ License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for full details.

